cat("P(B|+) =", p_plus_given_B_1, "\n")
cat("P(C|+) =", p_plus_given_C_1, "\n")
cat("P(A|-) =", p_minus_given_A_1, "\n")
cat("P(B|-) =", p_minus_given_B_1, "\n")
cat("P(C|-) =", p_minus_given_C_1, "\n")
# When the features are 0
# P(+|A=0)
p_plus_given_A_0 <- 1 - p_plus_given_A_1
# P(+|B=0)
p_plus_given_B_0 <- 1 - p_plus_given_B_1
# P(+|C=0)
p_plus_given_C_0 <- 1 - p_plus_given_C_1
# P(-|A=0)
p_minus_given_A_0 <- 1 - p_minus_given_A_1
# P(-|B=0)
p_minus_given_B_0 <- 1 - p_minus_given_B_1
# P(-|C=0)
p_minus_given_C_0 <- 1 - p_minus_given_C_1
# Print the results for 0
cat("P(+|A=0) =", p_plus_given_A_0, "\n")
cat("P(+|B=0) =", p_plus_given_B_0, "\n")
cat("P(+|C=0) =", p_plus_given_C_0, "\n")
cat("P(-|A=0) =", p_minus_given_A_0, "\n")
cat("P(-|B=0) =", p_minus_given_B_0, "\n")
cat("P(-|C=0) =", p_minus_given_C_0, "\n")
# Part B
# Test sample features
A_test <- 0
B_test <- 1
C_test <- 0
# Applying Naive Bayes approach
p_plus <- p_plus_given_A_1 * p_plus_given_B_1 * p_plus_given_C_1
p_minus <- p_minus_given_A_1 * p_minus_given_B_1 * p_minus_given_C_1
# Prior probabilities
prior_plus <- 0.5
prior_minus <- 0.5
# Considering the test sample features
if (A_test == 0) {
p_plus <- p_plus * (1 - p_plus_given_A_1)
p_minus <- p_minus * (1 - p_minus_given_A_1)
}
if (B_test == 1) {
p_plus <- p_plus * p_plus_given_B_1
p_minus <- p_minus * p_minus_given_B_1
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_plus_given_C_1)
p_minus <- p_minus * (1 - p_minus_given_C_1)
}
# Calculate the unnormalized posterior probabilities
unnormalized_posterior_plus <- p_plus * prior_plus
unnormalized_posterior_minus <- p_minus * prior_minus
# Predict the class label for the test sample
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
# Part C
# Given data
m <- 4
p <- 1/2
# Function to calculate the m-estimate
m_estimate <- function(n, p_event) {
(m * p + n * p_event) / (m + n)
}
# Count occurrences in the data
n_plus <- sum(df$Class == '+')
n_minus <- sum(df$Class == '-')
# Estimate conditional probabilities for A, B, and C when features are 1
p_plus_given_A_1_m <- m_estimate(n_plus, p_plus_given_A_1)
p_plus_given_B_1_m <- m_estimate(n_plus, p_plus_given_B_1)
p_plus_given_C_1_m <- m_estimate(n_plus, p_plus_given_C_1)
p_minus_given_A_1_m <- m_estimate(n_minus, p_minus_given_A_1)
p_minus_given_B_1_m <- m_estimate(n_minus, p_minus_given_B_1)
p_minus_given_C_1_m <- m_estimate(n_minus, p_minus_given_C_1)
# Estimate conditional probabilities for A, B, and C when features are 0
p_plus_given_A_0_m <- m_estimate(n_plus, 1 - p_plus_given_A_1)
p_plus_given_B_0_m <- m_estimate(n_plus, 1 - p_plus_given_B_1)
p_plus_given_C_0_m <- m_estimate(n_plus, 1 - p_plus_given_C_1)
p_minus_given_A_0_m <- m_estimate(n_minus, 1 - p_minus_given_A_1)
p_minus_given_B_0_m <- m_estimate(n_minus, 1 - p_minus_given_B_1)
p_minus_given_C_0_m <- m_estimate(n_minus, 1 - p_minus_given_C_1)
# Print the results
cat("P(+|A=1) using m-estimate =", p_plus_given_A_1_m, "\n")
cat("P(+|B=1) using m-estimate =", p_plus_given_B_1_m, "\n")
cat("P(+|C=1) using m-estimate =", p_plus_given_C_1_m, "\n")
cat("P(-|A=1) using m-estimate =", p_minus_given_A_1_m, "\n")
cat("P(-|B=1) using m-estimate =", p_minus_given_B_1_m, "\n")
cat("P(-|C=1) using m-estimate =", p_minus_given_C_1_m, "\n")
cat("P(+|A=0) using m-estimate =", p_plus_given_A_0_m, "\n")
cat("P(+|B=0) using m-estimate =", p_plus_given_B_0_m, "\n")
cat("P(+|C=0) using m-estimate =", p_plus_given_C_0_m, "\n")
cat("P(-|A=0) using m-estimate =", p_minus_given_A_0_m, "\n")
cat("P(-|B=0) using m-estimate =", p_minus_given_B_0_m, "\n")
cat("P(-|C=0) using m-estimate =", p_minus_given_C_0_m, "\n")
# Part D
# Applying Naive Bayes approach
p_plus <- p_plus_given_A_1_m * p_plus_given_B_1_m * p_plus_given_C_1_m
p_minus <- p_minus_given_A_1_m * p_minus_given_B_1_m * p_minus_given_C_1_m
# Prior probabilities
prior_plus <- 0.5
prior_minus <- 0.5
# Considering the test sample features
if (A_test == 0) {
p_plus <- p_plus * (1 - p_plus_given_A_1)
p_minus <- p_minus * (1 - p_minus_given_A_1)
}
if (B_test == 1) {
p_plus <- p_plus * p_plus_given_B_1
p_minus <- p_minus * p_minus_given_B_1
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_plus_given_C_1)
p_minus <- p_minus * (1 - p_minus_given_C_1)
}
# Calculate the unnormalized posterior probabilities
unnormalized_posterior_plus <- p_plus * prior_plus
unnormalized_posterior_minus <- p_minus * prior_minus
# Predict the class label for the test sample
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
# Part E
# The m-estimate is generally considered a better approach compared to MLE, especially
# when the dataset is limited or when there is prior information available.
# It provides a way to incorporate prior beliefs and prevents extreme estimates,
# making it more robust and reliable in various real-world scenarios.
# Define record IDs
Record_IDs <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
# Define feature A
Feature_A <- c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)
# Define feature B
Feature_B <- c(0, 0, 1, 1, 0, 0, 0, 0, 1, 0)
# Define feature C
Feature_C <- c(0, 1, 1, 1, 1, 1, 1, 1, 1, 1)
# Define class labels
Class_Labels <- c("+", "-", "-", "-", "+", "+", "-", "-", "+", "+")
# Create a dataframe
dataset <- data.frame(Record_IDs, Feature_A, Feature_B, Feature_C, Class_Labels)
# Calculate conditional probabilities for features A, B, and C given class '+'
p_A_given_plus <- sum(dataset$Feature_A[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
p_B_given_plus <- sum(dataset$Feature_B[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
p_C_given_plus <- sum(dataset$Feature_C[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
# Calculate conditional probabilities for features A, B, and C given class '-'
p_A_given_minus <- sum(dataset$Feature_A[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
p_B_given_minus <- sum(dataset$Feature_B[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
p_C_given_minus <- sum(dataset$Feature_C[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
# Print conditional probabilities
cat("P(A|+) =", p_A_given_plus, "\n")
cat("P(B|+) =", p_B_given_plus, "\n")
cat("P(C|+) =", p_C_given_plus, "\n")
cat("P(A|-) =", p_A_given_minus, "\n")
cat("P(B|-) =", p_B_given_minus, "\n")
cat("P(C|-) =", p_C_given_minus, "\n")
# Calculate conditional probabilities for when features are 0
p_plus_given_A_0 <- 1 - p_A_given_plus
p_plus_given_B_0 <- 1 - p_B_given_plus
p_plus_given_C_0 <- 1 - p_C_given_plus
p_minus_given_A_0 <- 1 - p_A_given_minus
p_minus_given_B_0 <- 1 - p_B_given_minus
p_minus_given_C_0 <- 1 - p_C_given_minus
# Print results for when features are 0
cat("P(+|A=0) =", p_plus_given_A_0, "\n")
cat("P(+|B=0) =", p_plus_given_B_0, "\n")
cat("P(+|C=0) =", p_plus_given_C_0, "\n")
cat("P(-|A=0) =", p_minus_given_A_0, "\n")
cat("P(-|B=0) =", p_minus_given_B_0, "\n")
cat("P(-|C=0) =", p_minus_given_C_0, "\n")
# Test sample features
A_test <- 0
B_test <- 1
C_test <- 0
# Update conditional probabilities based on test sample features
p_plus <- p_A_given_plus * p_B_given_plus * p_C_given_plus
p_minus <- p_A_given_minus * p_B_given_minus * p_C_given_minus
if (A_test == 0) {
p_plus <- p_plus * (1 - p_A_given_plus)
p_minus <- p_minus * (1 - p_A_given_minus)
}
if (B_test == 1) {
p_plus <- p_plus * p_B_given_plus
p_minus <- p_minus * p_B_given_minus
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_C_given_plus)
p_minus <- p_minus * (1 - p_C_given_minus)
}
# Calculate unnormalized posterior probabilities
unnormalized_posterior_plus <- p_plus * 0.5
unnormalized_posterior_minus <- p_minus * 0.5
# Predict class label for the test sample
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
# Calculate m-estimates for conditional probabilities
m_estimate <- function(n, p_event) {
(4 * 0.5 + n * p_event) / (4 + n)
}
n_plus <- sum(dataset$Class_Labels == '+')
n_minus <- sum(dataset$Class_Labels == '-')
p_plus_given_A_1_m <- m_estimate(n_plus, p_A_given_plus)
p_plus_given_B_1_m <- m_estimate(n_plus, p_B_given_plus)
p_plus_given_C_1_m <- m_estimate(n_plus, p_C_given_plus)
p_minus_given_A_1_m <- m_estimate(n_minus, p_A_given_minus)
p_minus_given_B_1_m <- m_estimate(n_minus, p_B_given_minus)
p_minus_given_C_1_m <- m_estimate(n_minus, p_C_given_minus)
p_plus_given_A_0_m <- m_estimate(n_plus, 1 - p_A_given_plus)
p_plus_given_B_0_m <- m_estimate(n_plus, 1 - p_B_given_plus)
p_plus_given_C_0_m <- m_estimate(n_plus, 1 - p_C_given_plus)
p_minus_given_A_0_m <- m_estimate(n_minus, 1 - p_A_given_minus)
p_minus_given_B_0_m <- m_estimate(n_minus, 1 - p_B_given_minus)
p_minus_given_C_0_m <- m_estimate(n_minus, 1 - p_C_given_minus)
# Print m-estimate results
cat("P(+|A=1) using m-estimate =", p_plus_given_A_1_m, "\n")
cat("P(+|B=1) using m-estimate =", p_plus_given_B_1_m, "\n")
cat("P(+|C=1) using m-estimate =", p_plus_given_C_1_m, "\n")
cat("P(-|A=1) using m-estimate =", p_minus_given_A_1_m, "\n")
cat("P(-|B=1) using m-estimate =", p_minus_given_B_1_m, "\n")
cat("P(-|C=1) using m-estimate =", p_minus_given_C_1_m, "\n")
cat("P(+|A=0) using m-estimate =", p_plus_given_A_0_m, "\n")
cat("P(+|B=0) using m-estimate =", p_plus_given_B_0_m, "\n")
cat("P(+|C=0) using m-estimate =", p_plus_given_C_0_m, "\n")
cat("P(-|A=0) using m-estimate =", p_minus_given_A_0_m, "\n")
cat("P(-|B=0) using m-estimate =", p_minus_given_B_0_m, "\n")
cat("P(-|C=0) using m-estimate =", p_minus_given_C_0_m, "\n")
# Reapply Naive Bayes approach with m-estimates
p_plus <- p_plus_given_A_1_m * p_plus_given_B_1_m * p_plus_given_C_1_m
p_minus <- p_minus_given_A_1_m * p_minus_given_B_1_m * p_minus_given_C_1_m
# Consider test sample features
if (A_test == 0) {
p_plus <- p_plus * (1 - p_A_given_plus)
p_minus <- p_minus * (1 - p_A_given_minus)
}
if (B_test == 1) {
p_plus <- p_plus * p_B_given_plus
p_minus <- p_minus * p_B_given_minus
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_C_given_plus)
p_minus <- p_minus * (1 - p_C_given_minus)
}
# Calculate unnormalized posterior probabilities with m-estimates
unnormalized_posterior_plus <- p_plus * 0.5
unnormalized_posterior_minus <- p_minus * 0.5
# Predict class label for the test sample with m-estimates
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
# Explanation of m-estimate
# The m-estimate is often preferred over Maximum Likelihood Estimation (MLE) when there's limited data or prior information available.
# It combines prior beliefs with observed data, providing more robust estimates, particularly in real-world scenarios.
# Define record IDs
Record_IDs <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
# Define feature A
Feature_A <- c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)
# Define feature B
Feature_B <- c(0, 0, 1, 1, 0, 0, 0, 0, 1, 0)
# Define feature C
Feature_C <- c(0, 1, 1, 1, 1, 1, 1, 1, 1, 1)
# Define class labels
Class_Labels <- c("+", "-", "-", "-", "+", "+", "-", "-", "+", "+")
# Create a dataframe
dataset <- data.frame(Record_IDs, Feature_A, Feature_B, Feature_C, Class_Labels)
# Calculate conditional probabilities for features A, B, and C given class '+'
p_A_given_plus <- sum(dataset$Feature_A[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
p_B_given_plus <- sum(dataset$Feature_B[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
p_C_given_plus <- sum(dataset$Feature_C[dataset$Class_Labels == '+']) / sum(dataset$Class_Labels == '+')
# Calculate conditional probabilities for features A, B, and C given class '-'
p_A_given_minus <- sum(dataset$Feature_A[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
p_B_given_minus <- sum(dataset$Feature_B[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
p_C_given_minus <- sum(dataset$Feature_C[dataset$Class_Labels == '-']) / sum(dataset$Class_Labels == '-')
# Print conditional probabilities
cat("P(A|+) =", p_A_given_plus, "\n")
cat("P(B|+) =", p_B_given_plus, "\n")
cat("P(C|+) =", p_C_given_plus, "\n")
cat("P(A|-) =", p_A_given_minus, "\n")
cat("P(B|-) =", p_B_given_minus, "\n")
cat("P(C|-) =", p_C_given_minus, "\n")
# Calculate conditional probabilities for when features are 0
p_plus_given_A_0 <- 1 - p_A_given_plus
p_plus_given_B_0 <- 1 - p_B_given_plus
p_plus_given_C_0 <- 1 - p_C_given_plus
p_minus_given_A_0 <- 1 - p_A_given_minus
p_minus_given_B_0 <- 1 - p_B_given_minus
p_minus_given_C_0 <- 1 - p_C_given_minus
# Print results for when features are 0
cat("P(+|A=0) =", p_plus_given_A_0, "\n")
cat("P(+|B=0) =", p_plus_given_B_0, "\n")
cat("P(+|C=0) =", p_plus_given_C_0, "\n")
cat("P(-|A=0) =", p_minus_given_A_0, "\n")
cat("P(-|B=0) =", p_minus_given_B_0, "\n")
cat("P(-|C=0) =", p_minus_given_C_0, "\n")
# Test sample features
A_test <- 0
B_test <- 1
C_test <- 0
# Update conditional probabilities based on test sample features
p_plus <- p_A_given_plus * p_B_given_plus * p_C_given_plus
p_minus <- p_A_given_minus * p_B_given_minus * p_C_given_minus
if (A_test == 0) {
p_plus <- p_plus * (1 - p_A_given_plus)
p_minus <- p_minus * (1 - p_A_given_minus)
}
if (B_test == 1) {
p_plus <- p_plus * p_B_given_plus
p_minus <- p_minus * p_B_given_minus
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_C_given_plus)
p_minus <- p_minus * (1 - p_C_given_minus)
}
# Calculate unnormalized posterior probabilities
unnormalized_posterior_plus <- p_plus * 0.5
unnormalized_posterior_minus <- p_minus * 0.5
# Predict class label for the test sample
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
# Calculate m-estimates for conditional probabilities
m_estimate <- function(n, p_event) {
(4 * 0.5 + n * p_event) / (4 + n)
}
n_plus <- sum(dataset$Class_Labels == '+')
n_minus <- sum(dataset$Class_Labels == '-')
p_plus_given_A_1_m <- m_estimate(n_plus, p_A_given_plus)
p_plus_given_B_1_m <- m_estimate(n_plus, p_B_given_plus)
p_plus_given_C_1_m <- m_estimate(n_plus, p_C_given_plus)
p_minus_given_A_1_m <- m_estimate(n_minus, p_A_given_minus)
p_minus_given_B_1_m <- m_estimate(n_minus, p_B_given_minus)
p_minus_given_C_1_m <- m_estimate(n_minus, p_C_given_minus)
p_plus_given_A_0_m <- m_estimate(n_plus, 1 - p_A_given_plus)
p_plus_given_B_0_m <- m_estimate(n_plus, 1 - p_B_given_plus)
p_plus_given_C_0_m <- m_estimate(n_plus, 1 - p_C_given_plus)
p_minus_given_A_0_m <- m_estimate(n_minus, 1 - p_A_given_minus)
p_minus_given_B_0_m <- m_estimate(n_minus, 1 - p_B_given_minus)
p_minus_given_C_0_m <- m_estimate(n_minus, 1 - p_C_given_minus)
# Print m-estimate results
cat("P(+|A=1) using m-estimate =", p_plus_given_A_1_m, "\n")
cat("P(+|B=1) using m-estimate =", p_plus_given_B_1_m, "\n")
cat("P(+|C=1) using m-estimate =", p_plus_given_C_1_m, "\n")
cat("P(-|A=1) using m-estimate =", p_minus_given_A_1_m, "\n")
cat("P(-|B=1) using m-estimate =", p_minus_given_B_1_m, "\n")
cat("P(-|C=1) using m-estimate =", p_minus_given_C_1_m, "\n")
cat("P(+|A=0) using m-estimate =", p_plus_given_A_0_m, "\n")
cat("P(+|B=0) using m-estimate =", p_plus_given_B_0_m, "\n")
cat("P(+|C=0) using m-estimate =", p_plus_given_C_0_m, "\n")
cat("P(-|A=0) using m-estimate =", p_minus_given_A_0_m, "\n")
cat("P(-|B=0) using m-estimate =", p_minus_given_B_0_m, "\n")
cat("P(-|C=0) using m-estimate =", p_minus_given_C_0_m, "\n")
# Reapply Naive Bayes approach with m-estimates
p_plus <- p_plus_given_A_1_m * p_plus_given_B_1_m * p_plus_given_C_1_m
p_minus <- p_minus_given_A_1_m * p_minus_given_B_1_m * p_minus_given_C_1_m
# Consider test sample features
if (A_test == 0) {
p_plus <- p_plus * (1 - p_A_given_plus)
p_minus <- p_minus * (1 - p_A_given_minus)
}
if (B_test == 1) {
p_plus <- p_plus * p_B_given_plus
p_minus <- p_minus * p_B_given_minus
}
if (C_test == 0) {
p_plus <- p_plus * (1 - p_C_given_plus)
p_minus <- p_minus * (1 - p_C_given_minus)
}
# Calculate unnormalized posterior probabilities with m-estimates
unnormalized_posterior_plus <- p_plus * 0.5
unnormalized_posterior_minus <- p_minus * 0.5
# Predict class label for the test sample with m-estimates
if (unnormalized_posterior_plus > unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is + (Positive).")
} else if (unnormalized_posterior_plus < unnormalized_posterior_minus) {
cat("The predicted class label for the test sample is - (Negative).")
} else {
cat("The probabilities for both classes are equal. Cannot make a prediction.")
}
print("Explanation of m-estimate \n")
print("The m-estimate is often preferred over Maximum Likelihood Estimation (MLE) when there's limited data or prior information available.\n")
print("It combines prior beliefs with observed data, providing more robust estimates, particularly in real-world scenarios.\n")
# Part A
undergrad_smoke_rate <- 0.15  # Smoking rate among undergraduate students
grad_smoke_rate <- 0.23  # Smoking rate among graduate students
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Step 1: Calculate total smoking proportion among all students
total_smoking_prop <- (prop_grad_students * grad_smoke_rate) + ((1 - prop_grad_students) * undergrad_smoke_rate)
# Step 2: Determine probability of randomly selecting a graduate student
prob_grad_student <- prop_grad_students
# Step 3: Apply Bayes' theorem to find probability of a smoker being a graduate student
prob_smoker_is_grad <- (prob_grad_student * grad_smoke_rate) / total_smoking_prop
# Output the result
print(prob_smoker_is_grad)
# Part B
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Calculate probability of selecting an undergraduate student
prob_undergrad_student <- 1 - prop_grad_students
# Calculate probability of selecting a graduate student
prob_grad_student <- prop_grad_students
# Compare probabilities
if (prob_grad_student > prob_undergrad_student) {
print("A randomly chosen college student is more likely to be a graduate student.")
} else if (prob_grad_student < prob_undergrad_student) {
print("A randomly chosen college student is more likely to be an undergraduate student.")
} else {
print("The probabilities of choosing a graduate student and an undergraduate student are equal.")
}
# Part C
undergrad_smoke_rate <- 0.15  # Smoking rate among undergraduate students
grad_smoke_rate <- 0.23  # Smoking rate among graduate students
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Calculate probability of selecting an undergraduate smoker
prob_undergrad_smoker <- (1 - prop_grad_students) * undergrad_smoke_rate
# Calculate probability of selecting a graduate smoker
prob_grad_smoker <- prop_grad_students * grad_smoke_rate
# Compare probabilities
if (prob_grad_smoker > prob_undergrad_smoker) {
print("A randomly chosen smoking college student is more likely to be a graduate student.")
} else if (prob_grad_smoker < prob_undergrad_smoker) {
print("A randomly chosen smoking college student is more likely to be an undergraduate student.")
} else {
print("The probabilities of choosing a graduate smoking student and an undergraduate smoking student are equal.")
}
# Part D
grad_dorm_rate <- 0.3  # Proportion of graduate students who live in a dorm
undergrad_dorm_rate <- 0.1  # Proportion of undergraduate students who live in a dorm
# Calculate probability of a smoking student in a dorm being a graduate student
prob_grad_smoker_dorm <- grad_dorm_rate * grad_smoke_rate
# Calculate probability of a smoking student in a dorm being an undergraduate student
prob_undergrad_smoker_dorm <- undergrad_dorm_rate * undergrad_smoke_rate
# Compare probabilities
if (prob_grad_smoker_dorm > prob_undergrad_smoker_dorm) {
print("A smoking student living in a dorm is more likely to be a graduate student.")
} else if (prob_grad_smoker_dorm < prob_undergrad_smoker_dorm) {
print("A smoking student living in a dorm is more likely to be an undergraduate student.")
} else {
print("The probabilities of a smoking student living in a dorm being a graduate student and an undergraduate student are equal.")
}
# Part A
undergrad_smoke_rate <- 0.15  # Smoking rate among undergraduate students
grad_smoke_rate <- 0.23  # Smoking rate among graduate students
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Step 1: Calculate total smoking proportion among all students
total_smoking_prop <- (prop_grad_students * grad_smoke_rate) + ((1 - prop_grad_students) * undergrad_smoke_rate)
# Step 2: Determine probability of randomly selecting a graduate student
prob_grad_student <- prop_grad_students
# Step 3: Apply Bayes' theorem to find probability of a smoker being a graduate student
prob_smoker_is_grad <- (prob_grad_student * grad_smoke_rate) / total_smoking_prop
# Output the result
print(prob_smoker_is_grad)
# Part B
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Calculate probability of selecting an undergraduate student
prob_undergrad_student <- 1 - prop_grad_students
# Calculate probability of selecting a graduate student
prob_grad_student <- prop_grad_students
# Compare probabilities
if (prob_grad_student > prob_undergrad_student) {
print("A randomly chosen college student is more likely to be a graduate student.")
} else if (prob_grad_student < prob_undergrad_student) {
print("A randomly chosen college student is more likely to be an undergraduate student.")
} else {
print("The probabilities of choosing a graduate student and an undergraduate student are equal.")
}
# Part C
undergrad_smoke_rate <- 0.15  # Smoking rate among undergraduate students
grad_smoke_rate <- 0.23  # Smoking rate among graduate students
prop_grad_students <- 1/5  # Proportion of students who are graduate students
# Calculate probability of selecting an undergraduate smoker
prob_undergrad_smoker <- (1 - prop_grad_students) * undergrad_smoke_rate
# Calculate probability of selecting a graduate smoker
prob_grad_smoker <- prop_grad_students * grad_smoke_rate
# Compare probabilities
if (prob_grad_smoker > prob_undergrad_smoker) {
print("A randomly chosen smoking college student is more likely to be a graduate student.")
} else if (prob_grad_smoker < prob_undergrad_smoker) {
print("A randomly chosen smoking college student is more likely to be an undergraduate student.")
} else {
print("The probabilities of choosing a graduate smoking student and an undergraduate smoking student are equal.")
}
# Part D
grad_dorm_rate <- 0.3  # Proportion of graduate students who live in a dorm
undergrad_dorm_rate <- 0.1  # Proportion of undergraduate students who live in a dorm
# Calculate probability of a smoking student in a dorm being a graduate student
prob_grad_smoker_dorm <- grad_dorm_rate * grad_smoke_rate
# Calculate probability of a smoking student in a dorm being an undergraduate student
prob_undergrad_smoker_dorm <- undergrad_dorm_rate * undergrad_smoke_rate
# Compare probabilities
if (prob_grad_smoker_dorm > prob_undergrad_smoker_dorm) {
print("A smoking student living in a dorm is more likely to be a graduate student.")
} else if (prob_grad_smoker_dorm < prob_undergrad_smoker_dorm) {
print("A smoking student living in a dorm is more likely to be an undergraduate student.")
} else {
print("The probabilities of a smoking student living in a dorm being a graduate student and an undergraduate student are equal.")
}
